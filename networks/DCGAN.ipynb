{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv2DTranspose,\n",
    "    Concatenate,\n",
    "    Convolution2D,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    LeakyReLU,\n",
    "    MaxPooling2D,\n",
    "    ReLU,\n",
    "    Reshape,\n",
    "    ZeroPadding2D\n",
    ")\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "EPS = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loss_validate import LossValidate\n",
    "from utils.batch_generator import BatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    \n",
    "    def __init__(self, img_rows, img_cols, batch_gen,\n",
    "                 save_model_dir, results_file, val_batch_size):\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.img_channels = 3\n",
    "        self.mask_channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
    "        self.mask_shape = (self.img_rows, self.img_cols, self.mask_channels)\n",
    "        self.discriminator_input_shape = (\n",
    "            self.img_rows, self.img_cols, self.img_channels + self.mask_channels\n",
    "        )\n",
    "        \n",
    "        self.batch_generator = batch_gen\n",
    "        \n",
    "        self.generator_weights_path = str(save_model_dir) + '/DCGAN_batch_1_epoch_{}.hdf5'\n",
    "        self.dcgan_weights_path = str(save_model_dir) + '/generator_batch_1_epoch_{}.hdf5'\n",
    "        self.discriminator_weights_path = str(save_model_dir) + '/discriminator_batch_1_epoch_{}.hdf5'\n",
    "        \n",
    "        results_file = os.path.join(save_model_dir, results_file)\n",
    "        \n",
    "        # build a generator\n",
    "        self.generator = self._generator(64)\n",
    "        self.generator.compile(\n",
    "            loss=DCGAN._generator_l1_loss,\n",
    "            optimizer=Adam(0.0002, 0.5)\n",
    "        )\n",
    "        \n",
    "        # build discriminator\n",
    "        self.discriminator = self._discriminator(64)\n",
    "        self.discriminator.compile(\n",
    "            loss=DCGAN._discriminator_loss,\n",
    "            optimizer=Adam(0.0002, 0.5),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # generator takes an image as an input and returns a mask\n",
    "        image = Input(shape=self.img_shape)\n",
    "        generated_mask = self.generator(image)\n",
    "    \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # The discriminator takes generated masks with images as input and determines validity\n",
    "        combined_inputs_fake = Concatenate(axis=-1)([image, generated_mask])\n",
    "        \n",
    "        # return patch o 1's and 0's\n",
    "        discrim_fake = self.discriminator(combined_inputs_fake)\n",
    "        self.combined_model = Model(inputs=image, outputs=combined_inputs_fake)\n",
    "        self.combined_model.compile(\n",
    "            optimizer=Adam(0.0002, 0.5),\n",
    "            loss=DCGAN._generator_loss(discrim_fake)\n",
    "        )\n",
    "        \n",
    "        # build logs\n",
    "        self.loss_validate = LossValidate(\n",
    "            self.generator,\n",
    "            batch_gen.generate_test_batch,\n",
    "            results_file,\n",
    "            val_batch_size=val_batch_size\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def _generator(self, n_filters):\n",
    "        \"\"\"Generator method\"\"\"\n",
    "        inputs = Input(shape=self.img_shape)\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        output = Convolution2D(\n",
    "            filters=n_filters, kernel_size=(2, 2),\n",
    "            strides=(2, 2), padding='same'\n",
    "        )(inputs)\n",
    "        layers.append(output)\n",
    "        \n",
    "        layers_specs = [\n",
    "            n_filters * 2,\n",
    "            n_filters * 4,\n",
    "            n_filters * 8,\n",
    "            n_filters * 8,\n",
    "            n_filters * 8,\n",
    "            n_filters * 8,\n",
    "            n_filters * 8\n",
    "        ]\n",
    "        \n",
    "        for output_channels in layers_specs:\n",
    "            rectified_inputs = LeakyReLU(alpha=0.2)(layers[-1])\n",
    "            convolved = Convolution2D(\n",
    "                filters=output_channels, kernel_size=4,\n",
    "                strides=(2, 2), padding='same'\n",
    "            )(rectified_inputs)\n",
    "            output = BatchNormalization(\n",
    "                axis=-1, momentum=0.1, epsilon=1e-5, gamma_initializer=RandomNormal(1.0, 0.02)\n",
    "            )(convolved)\n",
    "            layers.append(output)\n",
    "\n",
    "        layers_specs = [\n",
    "            (n_filters * 8, 0.5),\n",
    "            (n_filters * 8, 0.5),\n",
    "            (n_filters * 8, 0.5),\n",
    "            (n_filters * 8, 0.0),\n",
    "            (n_filters * 4, 0.0),\n",
    "            (n_filters * 2, 0.0),\n",
    "            (n_filters, 0.0),\n",
    "        ]\n",
    "\n",
    "        num_encoder_layers = len(layers)\n",
    "        for dec_layer, (output_channels, dropout) in enumerate(layers_specs):\n",
    "            skip_layer = num_encoder_layers - dec_layer - 1\n",
    "            if dec_layer == 0:\n",
    "                # no skip connection for the first decoding layer\n",
    "                layer_inputs = layers[-1]\n",
    "            else:\n",
    "                layer_inputs = Concatenate(axis=-1)([layers[-1], layers[skip_layer]])\n",
    "            rectified_output = ReLU()(layer_inputs)\n",
    "            output = Conv2DTranspose(\n",
    "                filters=output_channels, kernel_size=4, strides=(2, 2),\n",
    "                padding='same', kernel_initializer=RandomNormal(0.0, 0.02)\n",
    "            )(rectified_output)\n",
    "            output = BatchNormalization(\n",
    "                axis=-1, momentum=0.1,\n",
    "                epsilon=1e-5, gamma_initializer=RandomNormal(1.0, 0.02)\n",
    "            )(output)\n",
    "            \n",
    "            if dropout > 0:\n",
    "                output = Dropout(dropout)(output)\n",
    "            \n",
    "            layers.append(output)\n",
    "\n",
    "        layer_inputs = Concatenate(axis=-1)([layers[-1], layers[0]])\n",
    "        rectified_inputs = ReLU()(layer_inputs)\n",
    "        output = Conv2DTranspose(\n",
    "                filters=self.mask_channels, kernel_size=4, strides=(2, 2),\n",
    "                padding='same', kernel_initializer=RandomNormal(0.0, 0.02)\n",
    "        )(rectified_inputs)\n",
    "        output = Activation('tanh')(output)\n",
    "        layers.append(output)\n",
    "        return Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generator_l1_loss(targets, generated):\n",
    "        gen_loss_l1 = K.mean(K.abs(targets - generated))\n",
    "        return gen_loss_l1\n",
    "\n",
    "    @staticmethod\n",
    "    def _generator_loss(predict_fake,\n",
    "                        gan_weight=1.0,\n",
    "                        l1_weight=100):\n",
    "        # predict_fake => 1\n",
    "        # abs(targets - outputs) => 0\n",
    "        gen_loss_gan = K.mean(-K.log(predict_fake + EPS))\n",
    "\n",
    "        def loss(targets, generated):\n",
    "            gen_loss_l1 = K.mean(K.abs(targets - generated))\n",
    "            gen_loss = gen_loss_gan * gan_weight + gen_loss_l1 * l1_weight\n",
    "            return gen_loss\n",
    "        return loss\n",
    "        \n",
    "    def _discriminator(self, n_filters):\n",
    "        n_layers = 3\n",
    "        layers = []\n",
    "        \n",
    "        # 2x [batch, height, width, in_channels] =>\n",
    "        #    [batch, height, width, in_channels * 2]\n",
    "        combined_inputs = Input(shape=self.discriminator_input_shape)\n",
    "        padded = ZeroPadding2D(\n",
    "            padding=((1,1), (1, 1)), data_format='channels_last'\n",
    "        )(combined_inputs)\n",
    "        output = Convolution2D(\n",
    "            filters=n_filters, kernel_size=4,\n",
    "            strides=(2, 2), padding='valid',\n",
    "            kernel_initializer=RandomNormal(0, 0.02)\n",
    "        )(padded)\n",
    "        output = LeakyReLU(alpha=0.2)(output)\n",
    "        layers.append(output)\n",
    "        \n",
    "        # layer_2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]\n",
    "        # layer_3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]\n",
    "        # layer_4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            output_channels = n_filters * min(2 ** (i + 1), 8)\n",
    "            stride = 1 if i == n_layers - 1 else 2\n",
    "            output = Convolution2D(\n",
    "                filters=output_channels, kernel_size=4,\n",
    "                strides=(stride, stride), padding='valid',\n",
    "                kernel_initializer=RandomNormal(0, 0.02)\n",
    "            )(layers[-1])\n",
    "            output = BatchNormalization(\n",
    "                axis=-1, epsilon=1e-5, momentum=0.1, gamma_initializer=RandomNormal(1.0, 0.02)\n",
    "            )(output)\n",
    "            output = LeakyReLU(alpha=0.2)(output)\n",
    "            layers.append(output)\n",
    "        \n",
    "        output = Convolution2D(\n",
    "                filters=1, kernel_size=4,\n",
    "                strides=(1, 1), padding='valid',\n",
    "                kernel_initializer=RandomNormal(0, 0.02),\n",
    "                activation='sigmoid'\n",
    "            )(layers[-1])\n",
    "        layers.append(output)\n",
    "        \n",
    "        return Model(inputs=combined_inputs, outputs=layers[-1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def _discriminator_loss(predict_real, predict_fake):\n",
    "        # minimizing -tf.log will try to get inputs to 1\n",
    "        # predict_real => 1\n",
    "        # predict_fake => 0\n",
    "        return K.mean(\n",
    "            -(K.log(predict_real + EPS) + K.log(1 - predict_fake + EPS))\n",
    "        )\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def train(self, initial_epoch, nb_epochs, steps_per_epoch=int(1e3)):\n",
    "        \"\"\"Model training method\"\"\"\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((self.batch_generator.batch_size, 1))\n",
    "        fake = np.zeros((self.batch_generator.batch_size, 1))\n",
    "        \n",
    "        for epoch in range(initial_epoch + 1, initial_epoch + nb_epochs):\n",
    "            for step in range(steps_per_epoch):\n",
    "                # -------------------\n",
    "                # Train discriminator\n",
    "                # -------------------\n",
    "                img, mask = next(self.batch_generator.train_batches)\n",
    "                generated = self.generator.predict(img)\n",
    "\n",
    "                # train discriminator where real-like classified images are 1's\n",
    "                # and fake-like ones are 0's\n",
    "                discrim_loss_real = self.discriminator.train_on_batch(\n",
    "                    np.concatenate([img, mask], axis=-1), valid\n",
    "                )\n",
    "                d_loss_fake = self.discriminator.train_on_batch(\n",
    "                    np.concatenate([img, generated], axis=-1), fake\n",
    "                )\n",
    "\n",
    "                # ---------------\n",
    "                # Train generator\n",
    "                # ---------------\n",
    "                self.combined_model.train_on_batch(\n",
    "                    img, valid\n",
    "                )\n",
    "\n",
    "            logging.info('IOU: {:2f}'.format(DCGAN.metric(generated, mask)))\n",
    "            # log error value\n",
    "            self.loss_validate.error_log(epoch)\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                self.combined_model.save_weights(self.dcgan_weights_path)\n",
    "                self.generator.save_weights(self.generator_weights_path)\n",
    "                self.discriminator.save_weights(self.discriminator_weights_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def metric(y_true, y_false, smooth=1.):\n",
    "        y_true_f = y_true.flatten()\n",
    "        y_false_f = y_false.flatten()\n",
    "        intersection = np.sum(y_true_f * y_false_f)\n",
    "        union = np.sum(y_true_f) + np.sum(y_false_f) - intersection\n",
    "        return (intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "initial_epoch = 0\n",
    "batch_gen = BatchGenerator(\n",
    "    data_dir='../dataset/dataset_256', val_dir='../dataset/val_dataset_256', batch_size=BATCH_SIZE\n",
    ")\n",
    "batch_gen.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCGAN(256, 256, batch_gen, '../test_DCGAN', 'loss_dcgan.txt', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer model_59 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([], dtype=float32)]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    473\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[0;32m--> 474\u001b[0;31m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'numpy.ndarray'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-f337bd36752e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNB_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-140-1c723f1b4f69>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, initial_epoch, nb_epochs, steps_per_epoch)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# train discriminator where real-like classified images are 1's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# with the input_spec set at build time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Handle mask propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    283\u001b[0m                                  \u001b[0;34m'Received type: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full input: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. All inputs to the layer '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m                                  'should be tensors.')\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer model_59 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([], dtype=float32)]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "model.train(initial_epoch, NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=float32), array([], dtype=float32))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(batch_gen.train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4717"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gen.num_batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
